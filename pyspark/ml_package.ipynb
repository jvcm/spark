{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.43.125:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[3] appName=PySparkShell>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move to the ML part of Spark that operates\n",
    "strictly on DataFrames. Also, according to the Spark documentation, the primary\n",
    "machine learning API for Spark is now the DataFrame-based set of models contained\n",
    "in the spark.ml package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer class, like the name suggests, transforms your data by (normally)\n",
    "appending a new column to your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many Transformers offered in the spark.ml.feature and we will briefly\n",
    "describe them here (before we use some of them later in this chapter):\n",
    "\n",
    "- Binarizer: Given a threshold, the method takes a continuous variable and transforms it into a binary one.\n",
    "- Bucketizer: Similar to the Binarizer, this method takes a list of thresholds (the splits parameter) and transforms a continuous variable into a multinomial one.\n",
    "- MinMaxScaler: This is similar to the MaxAbsScaler with the difference that it scales the data to be in the [0.0, 1.0] range.\n",
    "- Normalizer: This method scales the data to be of unit norm using the p-norm value (by default, it is L2).\n",
    "- OneHotEncoder: This method encodes a categorical column to a column of binary vectors.\n",
    "- PCA: Performs the data reduction using principal component analysis.\n",
    "- StandardScaler: Standardizes the column to have a 0 mean and standard deviation equal to 1.\n",
    "- StopWordsRemover: Removes stop words (such as 'the' or 'a') from a tokenized text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators can be thought of as statistical models that need to be estimated to make\n",
    "predictions or classify your observations. Estimators can be divided into three distinct groups of problems: Classification (Supervised), Regression (Supervised) and Clustering (Unsupervised)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline in PySpark ML is a concept of an end-to-end transformation-estimation\n",
    "process (with distinct stages) that ingests some raw data (in a DataFrame form),\n",
    "performs the necessary data carpentry (transformations), and finally estimates a\n",
    "statistical model (estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the chances of infant survival with ML package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthsFilePath = './data/births_transformed.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = spark.read.csv(birthsFilePath,\n",
    "                       header = True,\n",
    "                       inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INFANT_ALIVE_AT_REPORT: integer (nullable = true)\n",
      " |-- BIRTH_PLACE: integer (nullable = true)\n",
      " |-- MOTHER_AGE_YEARS: integer (nullable = true)\n",
      " |-- FATHER_COMBINED_AGE: integer (nullable = true)\n",
      " |-- CIG_BEFORE: integer (nullable = true)\n",
      " |-- CIG_1_TRI: integer (nullable = true)\n",
      " |-- CIG_2_TRI: integer (nullable = true)\n",
      " |-- CIG_3_TRI: integer (nullable = true)\n",
      " |-- MOTHER_HEIGHT_IN: integer (nullable = true)\n",
      " |-- MOTHER_PRE_WEIGHT: integer (nullable = true)\n",
      " |-- MOTHER_DELIVERY_WEIGHT: integer (nullable = true)\n",
      " |-- MOTHER_WEIGHT_GAIN: integer (nullable = true)\n",
      " |-- DIABETES_PRE: integer (nullable = true)\n",
      " |-- DIABETES_GEST: integer (nullable = true)\n",
      " |-- HYP_TENS_PRE: integer (nullable = true)\n",
      " |-- HYP_TENS_GEST: integer (nullable = true)\n",
      " |-- PREV_BIRTH_PRETERM: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize <i>BIRTH_PLACE</i> numeric variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ft.OneHotEncoder(inputCol = 'BIRTH_PLACE',\n",
    "                          outputCol = 'BIRTH_PLACE_VEC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single column with all the features collated together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols = births.columns[2:] +\\\n",
    "    [encoder.getOutputCol()],\n",
    "    outputCol = 'features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating an estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(featuresCol = 'features',\n",
    "            maxIter = 10,\n",
    "            regParam = 0.01,\n",
    "            labelCol = 'INFANT_ALIVE_AT_REPORT'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the pipeline should look like conceptually:\n",
    "\n",
    "<img src=\"imgs/pipeline.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages = [encoder,\n",
    "             featuresCreator,\n",
    "             logistic]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_train, births_test = births.randomSplit(\n",
    "    [0.7, 0.3], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(births_train)\n",
    "test_model = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+----------+\n",
      "|INFANT_ALIVE_AT_REPORT|BIRTH_PLACE|MOTHER_AGE_YEARS|FATHER_COMBINED_AGE|CIG_BEFORE|CIG_1_TRI|CIG_2_TRI|CIG_3_TRI|MOTHER_HEIGHT_IN|MOTHER_PRE_WEIGHT|MOTHER_DELIVERY_WEIGHT|MOTHER_WEIGHT_GAIN|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|BIRTH_PLACE_VEC|            features|       rawPrediction|         probability|prediction|\n",
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+----------+\n",
      "|                     0|          1|              12|                 99|         0|        0|        0|        0|              62|              145|                   152|                 7|           0|            0|           0|            0|                 0|  (9,[1],[1.0])|(24,[0,1,6,7,8,9,...|[0.94887004595139...|[0.72088787836923...|       0.0|\n",
      "|                     0|          1|              13|                 99|         0|        0|        0|        0|              57|              100|                   108|                 8|           0|            0|           0|            0|                 0|  (9,[1],[1.0])|(24,[0,1,6,7,8,9,...|[0.70765277604636...|[0.66988230182926...|       0.0|\n",
      "|                     0|          1|              13|                 99|         0|        0|        0|        0|              62|              218|                   240|                22|           0|            0|           0|            0|                 0|  (9,[1],[1.0])|(24,[0,1,6,7,8,9,...|[0.87789428454370...|[0.70638567515772...|       0.0|\n",
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol = 'probability',\n",
    "    labelCol = 'INFANT_ALIVE_AT_REPORT'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7426157878507577\n",
      "0.7141632790662186\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test_model,\n",
    "                         {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test_model,\n",
    "                         {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark allows you to save the Pipeline definition for later use. It not only saves\n",
    "the pipeline structure, but also all the definitions of all the Transformers and\n",
    "Estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = './data/infant_oneHotEncoder_Logistic_Pipeline'\n",
    "pipeline.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedPipeline = Pipeline.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE=1, MOTHER_AGE_YEARS=12, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=62, MOTHER_PRE_WEIGHT=145, MOTHER_DELIVERY_WEIGHT=152, MOTHER_WEIGHT_GAIN=7, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 12.0, 1: 99.0, 6: 62.0, 7: 145.0, 8: 152.0, 9: 7.0, 16: 1.0}), rawPrediction=DenseVector([0.9489, -0.9489]), probability=DenseVector([0.7209, 0.2791]), prediction=0.0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedPipeline \\\n",
    " .fit(births_train)\\\n",
    " .transform(births_test)\\\n",
    " .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you, however, want to save the estimated model, you can also do that; instead of\n",
    "saving the Pipeline, you need to save the PipelineModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = './data/infant_oneHotEncoder_Logistic_PipelineModel'\n",
    "model.write().overwrite().save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedPipelineModel = PipelineModel.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE=1, MOTHER_AGE_YEARS=12, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=62, MOTHER_PRE_WEIGHT=145, MOTHER_DELIVERY_WEIGHT=152, MOTHER_WEIGHT_GAIN=7, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 12.0, 1: 99.0, 6: 62.0, 7: 145.0, 8: 152.0, 9: 7.0, 16: 1.0}), rawPrediction=DenseVector([0.9489, -0.9489]), probability=DenseVector([0.7209, 0.2791]), prediction=0.0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedPipelineModel\\\n",
    "    .transform(births_test).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter hyper-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concept of parameter hyper-tuning is to find the best parameters of the model: for\n",
    "example, the maximum number of iterations needed to properly estimate the logistic\n",
    "regression model or maximum depth of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid search:\n",
    "\n",
    "Grid search is an exhaustive algorithm that loops through the list of defined\n",
    "parameter values, estimates separate models, and chooses the best one given\n",
    "some evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(\n",
    "    labelCol = 'INFANT_ALIVE_AT_REPORT')\n",
    "grid = tune.ParamGridBuilder()\\\n",
    "    .addGrid(logistic.maxIter, [2, 10, 50])\\\n",
    "    .addGrid(logistic.regParam, [0.01, 0.05, 0.3])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol = 'probability',\n",
    "    labelCol = 'INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CrossValidator needs the estimator, the estimatorParamMaps, and the\n",
    "evaluator to do its job. The model loops through the grid of values, estimates\n",
    "the models, and compares their performance using the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(\n",
    "    estimator = logistic,\n",
    "    estimatorParamMaps = grid,\n",
    "    evaluator = evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIRTHS_PLACE still not encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [encoder, featuresCreator])\n",
    "data_transformer = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(data_transformer.transform(births_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use it to see if it\n",
    "performed better than our previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7434926434958825\n",
      "0.7144290754256393\n"
     ]
    }
   ],
   "source": [
    "data_train = data_transformer\\\n",
    "    .transform(births_test)\n",
    "results = cvModel.transform(data_train)\n",
    "print(evaluator.evaluate(results,\n",
    "    {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results,\n",
    "    {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'maxIter': 50}, {'regParam': 0.01}], 0.7411920462870111)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [\n",
    " (\n",
    " [\n",
    " {key.name: paramValue}\n",
    " for key, paramValue\n",
    " in zip(\n",
    " params.keys(),\n",
    " params.values())\n",
    " ], metric\n",
    " )\n",
    " for params, metric\n",
    " in zip(\n",
    " cvModel.getEstimatorParamMaps(),\n",
    " cvModel.avgMetrics\n",
    " )\n",
    "]\n",
    "sorted(results,\n",
    " key=lambda el: el[1],\n",
    " reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast label column as double type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = births.withColumn(\n",
    "    'INFANT_ALIVE_AT_REPORT',\n",
    "    fn.col('INFANT_ALIVE_AT_REPORT').cast(typ.DoubleType())\n",
    ")\n",
    "births_train, births_test = births.randomSplit(\n",
    "    [0.7, 0.3], seed = 666\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = cl.RandomForestClassifier(\n",
    "    numTrees = 10,\n",
    "    maxDepth = 5,\n",
    "    labelCol = 'INFANT_ALIVE_AT_REPORT'\n",
    ")\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        encoder,\n",
    "        featuresCreator,\n",
    "        classifier]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    labelCol = 'INFANT_ALIVE_AT_REPORT'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647280689555164\n",
      "0.7469692302466407\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test,\n",
    "    {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test,\n",
    "    {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = clus.KMeans(k = 5,\n",
    "                    featuresCol = 'features')\n",
    "pipeline = Pipeline(stages = [\n",
    "    encoder,\n",
    "    featuresCreator,\n",
    "    kmeans\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+--------+\n",
      "|prediction|avg(MOTHER_HEIGHT_IN)|count(1)|\n",
      "+----------+---------------------+--------+\n",
      "|         1|    83.91154791154791|     407|\n",
      "|         3|    66.64658634538152|     249|\n",
      "|         4|    64.31597357170618|   10292|\n",
      "|         2|    67.69473684210526|     475|\n",
      "|         0|    64.43472584856397|    2298|\n",
      "+----------+---------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clust = test.groupBy('prediction')\\\n",
    "    .agg({\n",
    "        '*': 'count',\n",
    "        'MOTHER_HEIGHT_IN': 'avg'\n",
    "})\n",
    "clust.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['MOTHER_AGE_YEARS','MOTHER_HEIGHT_IN',\n",
    " 'MOTHER_PRE_WEIGHT','DIABETES_PRE',\n",
    " 'DIABETES_GEST','HYP_TENS_PRE',\n",
    " 'HYP_TENS_GEST', 'PREV_BIRTH_PRETERM',\n",
    " 'CIG_BEFORE','CIG_1_TRI', 'CIG_2_TRI',\n",
    " 'CIG_3_TRI'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols = [col for col in features[1:]],\n",
    "    outputCol = 'features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the top six most important features using Chi Square Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(\n",
    "    numTopFeatures = 6,\n",
    "    outputCol = 'selectedFeatures',\n",
    "    labelCol = 'MOTHER_WEIGHT_GAIN'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the gradient boosted trees regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.regression as reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = reg.GBTRegressor(\n",
    "    maxIter = 15,\n",
    "    maxDepth = 3,\n",
    "    labelCol = 'MOTHER_WEIGHT_GAIN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [\n",
    "    featuresCreator,\n",
    "    selector,\n",
    "    regressor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightGain = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.RegressionEvaluator(\n",
    "    predictionCol = 'prediction',\n",
    "    labelCol = 'MOTHER_WEIGHT_GAIN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4883122086226992\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(\n",
    "    weightGain.transform(births_test),\n",
    "    {evaluator.metricName: 'r2'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
